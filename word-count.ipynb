{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b1b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "initial = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01fc9f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a49c31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName('Word Count') \\\n",
    "    .setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8179c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strings to be removed from text files\n",
    "import string\n",
    "remove = string.punctuation+string.digits+string.whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc615d71",
   "metadata": {},
   "source": [
    "##### Word Count for 124MB text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a698bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_124MB = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5901b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_124MB = sc.textFile('./input/leipzig124MB.txt')\n",
    "num_lines_124MB = rdd_124MB.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40222c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the text\n",
    "words_124MB = rdd_124MB \\\n",
    "    .flatMap(lambda line: line \\\n",
    "        .translate(str.maketrans(remove, ''.ljust(len(remove)))) \\\n",
    "        .lower() \\\n",
    "        .split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1298b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The filter function cleans '' values as a result of the normalization operation.\n",
    "wordCounts_124MB = words_124MB \\\n",
    "    .filter(lambda word: len(word) != 0) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a,b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbbd2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts_124MB_list = wordCounts_124MB.collect()\n",
    "unique_words_124MB = len(wordCounts_124MB_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f3e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_key_value_pairs_124MB = sorted(wordCounts_124MB_list, key=lambda item: item[1], reverse=True)\n",
    "sorted_first_10_key_value_pair_124MB = '\\n'.join(map(str, sorted_key_value_pairs_124MB[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4406adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output_124MB.txt', 'w') as f:\n",
    "    f.write(sorted_first_10_key_value_pair_124MB)\n",
    "end_124MB = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f010d2",
   "metadata": {},
   "source": [
    "##### Word Count for 12GB text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4a436d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_12GB = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a46596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_12GB = sc.textFile('./input/leipzig12GB.txt').coalesce(16)\n",
    "num_lines_12GB = rdd_12GB.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe27114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the text\n",
    "words_12GB = rdd_12GB \\\n",
    "    .flatMap(lambda line: line \\\n",
    "        .translate(str.maketrans(remove, ''.ljust(len(remove)))) \\\n",
    "        .lower() \\\n",
    "        .split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fe59759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The filter function cleans '' values as a result of the normalization operation.\n",
    "wordCounts_12GB = words_12GB \\\n",
    "    .filter(lambda word: len(word) != 0) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a,b: a + b).coalesce(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7930844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decreasing partition numbers. Logical processors(8) * 2 = 16\n",
    "wordCounts_12GB_list = wordCounts_12GB.coalesce(16).collect()\n",
    "unique_words_12GB = len(wordCounts_12GB_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b62577fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_key_value_pairs_12GB = sorted(wordCounts_12GB_list, key=lambda item: item[1], reverse=True)\n",
    "sorted_first_10_key_value_pair_12GB = '\\n'.join(map(str, sorted_key_value_pairs_12GB[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7130f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output_12GB.txt', 'w') as f:\n",
    "    f.write(sorted_first_10_key_value_pair_12GB)\n",
    "end_12GB = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "667c1392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prints for 124 MB text file\n",
      "--------------------------------------------------------------------\n",
      "Number of lines in leipzig124MB.txt file: 1000000 lines.\n",
      "Number of occurrences of unique words in leipzig124MB.txt: 164860 unique words.\n",
      "\n",
      "First 10 key-value pair of sorted result:\n",
      "('the', 1372271)\n",
      "('of', 597086)\n",
      "('to', 568300)\n",
      "('a', 513030)\n",
      "('in', 475365)\n",
      "('and', 449883)\n",
      "('s', 264098)\n",
      "('said', 215925)\n",
      "('for', 215232)\n",
      "('that', 206253)\n",
      "\n",
      "The processes for 124MB text file  took 0:00:16.026593\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(f'Prints for 124 MB text file', end='\\n--------------------------------------------------------------------\\n')\n",
    "print(f'Number of lines in leipzig124MB.txt file: {num_lines_124MB} lines.')\n",
    "print(f'Number of occurrences of unique words in leipzig124MB.txt: {unique_words_124MB} unique words.', end='\\n\\n')\n",
    "print(f'First 10 key-value pair of sorted result:\\n{sorted_first_10_key_value_pair_124MB}', end='\\n\\n')\n",
    "print(f'The processes for 124MB text file  took {datetime.timedelta(seconds=end_124MB - initial_124MB)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fd15dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prints for 12 GB text file\n",
      "--------------------------------------------------------------------\n",
      "Number of lines in leipzig12GB.txt file: 96000001 lines.\n",
      "Number of occurrences of unique words in leipzig12GB.txt: 164861 unique words.\n",
      "\n",
      "First 10 key-value pair of sorted result:\n",
      " ('the', 131738016)\n",
      "('of', 57320256)\n",
      "('to', 54556800)\n",
      "('a', 49250880)\n",
      "('in', 45635040)\n",
      "('and', 43188768)\n",
      "('s', 25353408)\n",
      "('said', 20728800)\n",
      "('for', 20662272)\n",
      "('that', 19800288)\n",
      "\n",
      "The processes for 12GB text file  took 0:14:30.145427\n"
     ]
    }
   ],
   "source": [
    "print(f'Prints for 12 GB text file', end='\\n--------------------------------------------------------------------\\n')\n",
    "print(f'Number of lines in leipzig12GB.txt file: {num_lines_12GB} lines.')\n",
    "print(f'Number of occurrences of unique words in leipzig12GB.txt: {unique_words_12GB} unique words.', end='\\n\\n')\n",
    "print(f'First 10 key-value pair of sorted result:\\n {sorted_first_10_key_value_pair_12GB}', end='\\n\\n')\n",
    "print(f'The processes for 12GB text file  took {datetime.timedelta(seconds=end_12GB - initial_12GB)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92c7dad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole process took 0:14:49.801940\n"
     ]
    }
   ],
   "source": [
    "print(f\"The whole process took {datetime.timedelta(seconds=end_12GB - initial)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
